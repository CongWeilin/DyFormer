{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from torch_geometric.nn.inits import uniform, glorot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, vector_length, hidden_size, memory_size):\n",
    "        super(NTM, self).__init__()\n",
    "        self.controller = FeedForwardController(vector_length = vector_length + 1 + memory_size[1], \n",
    "                                                hidden_size   = hidden_size)\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.read_head = ReadHead(self.memory, hidden_size)\n",
    "        self.write_head = WriteHead(self.memory, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size + memory_size[1], vector_length)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self,):\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "\n",
    "    def get_initial_state(self, batch_size=1):\n",
    "        self.memory.reset(batch_size)\n",
    "        read = self.memory.get_initial_read(batch_size)\n",
    "        read_head_state = self.read_head.get_initial_state(batch_size)\n",
    "        write_head_state = self.write_head.get_initial_state(batch_size)\n",
    "        return (read, read_head_state, write_head_state)\n",
    "\n",
    "    def forward(self, x, previous_state):\n",
    "        previous_read, previous_read_head_state, previous_write_head_state = previous_states\n",
    "        \n",
    "        controller_x = torch.cat([x, previous_read], dim=1)\n",
    "        controller_x = self.controller(controller_x)\n",
    "        # Read\n",
    "        read_head_output, read_head_state = self.read_head(controller_x, previous_read_head_state)\n",
    "        # Write\n",
    "        write_head_state = self.write_head(controller_x, previous_write_head_state)\n",
    "        \n",
    "        fc_input = torch.cat((controller_x, read_head_output), dim=1)\n",
    "        state = (read_head_output, read_head_state, write_head_state)\n",
    "        return F.sigmoid(self.fc(fc_input)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, memory_size):\n",
    "        super(Memory, self).__init__()\n",
    "        self._memory_size = memory_size\n",
    "\n",
    "        # Initialize memory bias\n",
    "        initial_state = torch.ones(memory_size) * 1e-6\n",
    "        self.register_buffer('initial_state', initial_state.data)\n",
    "\n",
    "        # Initial read vector is a learnt parameter\n",
    "        self.initial_read = nn.Parameter(torch.randn(1, self._memory_size[1]) * 0.01)\n",
    "\n",
    "    def get_size(self):\n",
    "        return self._memory_size\n",
    "\n",
    "    def reset(self, batch_size): # >>> Do we need batch size?\n",
    "        self.memory = self.initial_state.clone().repeat(batch_size, 1, 1)\n",
    "\n",
    "    def get_initial_read(self, batch_size):\n",
    "        return self.initial_read.clone().repeat(batch_size, 1)\n",
    "\n",
    "    def read(self):\n",
    "        return self.memory\n",
    "\n",
    "    def write(self, w, e, a):\n",
    "        self.memory = self.memory * (1 - torch.matmul(w.unsqueeze(-1), e.unsqueeze(1)))\n",
    "        self.memory = self.memory + torch.matmul(w.unsqueeze(-1), a.unsqueeze(1))\n",
    "        return self.memory\n",
    "\n",
    "    def size(self):\n",
    "        return self._memory_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, memory, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.memory = memory\n",
    "        memory_length, memory_vector_length = memory.get_size()\n",
    "        # (k : vector, beta: scalar, g: scalar, s: vector, gamma: scalar)\n",
    "        self.k_layer = nn.Linear(hidden_size, memory_vector_length)\n",
    "        self.beta_layer = nn.Linear(hidden_size, 1)\n",
    "        self.g_layer = nn.Linear(hidden_size, 1)\n",
    "        self.s_layer = nn.Linear(hidden_size, 3)\n",
    "        self.gamma_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        for layer in [self.k_layer, self.beta_layer, self.g_layer, self.s_layer, self.gamma_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "\n",
    "        self._initial_state = nn.Parameter(torch.randn(1, self.memory.get_size()[0]) * 1e-5)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        # Softmax to ensure weights are normalized\n",
    "        return F.softmax(self._initial_state, dim=1).repeat(batch_size, 1)\n",
    "\n",
    "    def get_head_weight(self, x, previous_state, memory_read):\n",
    "        k = self.k_layer(x)\n",
    "        beta = F.softplus(self.beta_layer(x))\n",
    "        g = F.sigmoid(self.g_layer(x))\n",
    "        s = F.softmax(self.s_layer(x), dim=1)\n",
    "        gamma = 1 + F.softplus(self.gamma_layer(x))\n",
    "        # Focusing by content\n",
    "        w_c = F.softmax(beta * F.cosine_similarity(memory_read + 1e-16, k.unsqueeze(1) + 1e-16, dim=-1), dim=1)\n",
    "        # Focusing by location\n",
    "        w_g = g * w_c + (1 - g) * previous_state\n",
    "        w_t = self.shift(w_g, s)\n",
    "        w = w_t ** gamma\n",
    "        w = torch.div(w, torch.sum(w, dim=1).unsqueeze(1) + 1e-16)\n",
    "        return w\n",
    "\n",
    "    def shift(self, w_g, s):\n",
    "        result = w_g.clone()\n",
    "        for b in range(len(w_g)):\n",
    "            result[b] = _convolve(w_g[b], s[b])\n",
    "        return result\n",
    "    \n",
    "def _convolve(w, s):\n",
    "    \"\"\"Circular convolution implementation.\"\"\"\n",
    "    assert s.size(0) == 3\n",
    "    t = torch.cat([w[-1:], w, w[:1]], dim=0)\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadHead(Head):\n",
    "    def forward(self, x, previous_state):\n",
    "        memory_read = self.memory.read()\n",
    "        w = self.get_head_weight(x, previous_state, memory_read)\n",
    "        return torch.matmul(w.unsqueeze(1), memory_read).squeeze(1), w\n",
    "\n",
    "\n",
    "class WriteHead(Head):\n",
    "    def __init__(self, memory, hidden_size):\n",
    "        super(WriteHead, self).__init__(memory, hidden_size)\n",
    "        memory_length, memory_vector_length = memory.get_size()\n",
    "        self.e_layer = nn.Linear(hidden_size, memory_vector_length)\n",
    "        self.a_layer = nn.Linear(hidden_size, memory_vector_length)\n",
    "        for layer in [self.e_layer, self.a_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, previous_state):\n",
    "        memory_read = self.memory.read()\n",
    "        w = self.get_head_weight(x, previous_state, memory_read)\n",
    "        e = F.sigmoid(self.e_layer(x))\n",
    "        a = self.a_layer(x)\n",
    "\n",
    "        # write to memory (w, memory, e , a)\n",
    "        self.memory.write(w, e, a)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Controller(nn.Module):\n",
    "#     def __init__(self, vector_length, hidden_size):\n",
    "#         super(Controller, self).__init__()\n",
    "#         # We allow either a feed-forward network or a LSTM for the controller\n",
    "#         self._controller = FeedForwardController(vector_length, hidden_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self._controller(x)\n",
    "\n",
    "#     def get_initial_state(self, batch_size):\n",
    "#         return self._controller.get_initial_state(batch_size)\n",
    "    \n",
    "class FeedForwardController(nn.Module):\n",
    "    def __init__(self, vector_length, hidden_size):\n",
    "        super(FeedForwardController, self).__init__()\n",
    "        self.vector_length = vector_length\n",
    "        self.hidden_size   = hidden_size\n",
    "        \n",
    "        self.layer_1 = nn.Linear(vector_length, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        \n",
    "    def reset_parameter():\n",
    "        stdev = 5 / (np.sqrt(self.vector_length + self.hidden_size))\n",
    "        nn.init.uniform_(self.layer_1.weight, -stdev, stdev)\n",
    "        nn.init.uniform_(self.layer_2.weight, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        return x\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1285,  0.7534,  0.2341],\n",
       "        [ 0.5962,  0.3908, -0.4896],\n",
       "        [ 1.9124,  0.1074,  0.5756],\n",
       "        [ 0.1425, -1.9427,  0.4533],\n",
       "        [-0.2537,  0.1529,  0.5604]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Embedding(5, 3, max_norm=2)\n",
    "a(torch.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
